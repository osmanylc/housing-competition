{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import cleaning as cl\n",
    "import tuning as tn\n",
    "\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffd2d8a76a618d9b5e78773072cdfdb8def4aa74"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "X_eval = pd.read_csv('test.csv')\n",
    "\n",
    "X = train_df.drop(columns='SalePrice')\n",
    "y = train_df.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up data for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_eval = cl.create_one_hot_encoding(X, y, X_eval)\n",
    "# X_t, X_v, y_t, y_v = train_test_split(X, y, \n",
    "#                                       test_size=.2,\n",
    "#                                       random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "models = {'rfr': RandomForestRegressor(n_estimators=100, \n",
    "                                       criterion='mae', \n",
    "                                       n_jobs=-1,\n",
    "                                       random_state=seed),\n",
    "          'xgbr': XGBRegressor(n_estimators=100,\n",
    "                               random_state=seed), \n",
    "          'skgbr': GradientBoostingRegressor(loss='lad', \n",
    "                                             n_estimators=100,\n",
    "                                             random_state=seed)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline models into estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "estimators = {name: make_pipeline(SimpleImputer(), model) \n",
    "              for name, model in models.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit estimators to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {'rfr': {},\n",
    "              'xgbr': {'xgbregressor__verbose': False}, \n",
    "              'skgbr': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in fit_params.items():\n",
    "    estimators[name].fit(X_t, y_t, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for name, estimator in estimators.items():\n",
    "    y_pred = estimator.predict(X_v)\n",
    "\n",
    "    print(mean_absolute_error(y_pred, y_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = {}\n",
    "\n",
    "for name, estimator in estimators.items():\n",
    "    cv_scores[name] = cross_val_score(estimator, X, y, cv=5,\n",
    "                                      scoring='neg_mean_absolute_error', fit_params=fit_params[name])\n",
    "    \n",
    "    print(f'{name} cv score: {-np.mean(cv_scores[name])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This works for cv yeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_params = {'xgbregressor__n_estimators': randint(low=1e2, high=1e3),\n",
    "               'xgbregressor__learning_rate': reciprocal(a=1e-2, b=1e-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RandomizedSearchCV(estimators['xgbr'], param_distributions=xgbr_params, \n",
    "                   n_iter=10, scoring='neg_mean_absolute_error', cv=5).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randint(1, 10).interval(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for auto param search\n",
    "\n",
    "1. Use `RandomizedSearchCV` to find best params over a range.\n",
    "\n",
    "2. Take results of `RandomizedSearchCV` to reduce area of grid to search.\n",
    "    + Search should happen around the best param, and the area should be reduced by a factor.\n",
    "    + Add parameter specifying number of iterations of `RandomizedSearchCV` to run, each with the reduced parameter grid area.\n",
    "\n",
    "3. Return the parameters, the score, and the model that is the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param_set(low, high, n, is_log=False, is_int=False):\n",
    "    if is_log:\n",
    "        low, high = [np.log(x) for x in [low, high]]\n",
    "    \n",
    "    step_size = (high - low) / (n-1)\n",
    "    param_set = [low + step_size * i for i in range(n)]\n",
    "    \n",
    "    if is_log:\n",
    "        param_set = [np.exp(p) for p in param_set]\n",
    "        \n",
    "    if is_int:\n",
    "        param_set = [int(p) for p in param_set]\n",
    "        \n",
    "    return param_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_param_sets(param_ranges):\n",
    "    param_sets = {}\n",
    "    for args in param_ranges:\n",
    "        param_set = make_param_set(args['low'], args['high'], args['n'], args['is_log'], args['is_int'])\n",
    "        param_sets[args['name']] = param_set\n",
    "    \n",
    "    return param_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyperparams_iterated(model, X, y, param_ranges, n_samplings, scoring, cv_folds, n_iters, alpha):\n",
    "    \"\"\"\n",
    "    Find best hyperparams based on cross-validation scores.\n",
    "    \n",
    "    param_ranges: list of param arguments to build a param set.\n",
    "    \"\"\"\n",
    "    param_sets = make_all_param_sets(param_ranges)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        rand_search_results = RandomizedSearchCV(model, param_distributions=param_distributions,\n",
    "                                                 n_iter=n_samplings, scoring=scoring, cv=cv_folds).fit(X,y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_all_param_sets([{'name': 'xgbregressor__n_estimators', \n",
    "                      'low': 1e2, \n",
    "                      'high': 1e3, \n",
    "                      'n': 10, \n",
    "                      'is_log': False, \n",
    "                      'is_int': True},\n",
    "                     {'name': 'xgbregressor__learning_rate',\n",
    "                      'low': 1e-2,\n",
    "                      'high': 1e-1,\n",
    "                      'n': 10,\n",
    "                      'is_log': True,\n",
    "                      'is_int': False}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_param_set(.01, .1, 10, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_param_range(low, high, param, alpha, is_int=True):\n",
    "    param_range = high - low\n",
    "    low, high = (param - alpha * param_range / 2, param + alpha * param_range / 2)\n",
    "    \n",
    "    if is_int:\n",
    "        low, high = [int(x) for x in [low, high]]\n",
    "        \n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_param_range(1, 1000, 900, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cv for hyperparameter tuning: XGBR\n",
    "\n",
    "Let's try to choose the optimal value for:\n",
    "\n",
    "1. `n_estimators`\n",
    "2. `learning_rate`\n",
    "\n",
    "TODO: Automate hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est_set = np.random.randint(600, 1000, 10)\n",
    "lr_set = np.random.uniform(.03, .06, 10)\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "for n_est, lr in zip(n_est_set, lr_set):\n",
    "    xgbr = make_pipeline(SimpleImputer(), \n",
    "                         XGBRegressor(n_estimators=n_est, learning_rate=lr, random_state=seed),\n",
    "                         memory=cachedir)\n",
    "    cv_score = cross_val_score(xgbr, X, y, cv=5,\n",
    "                               scoring='neg_mean_absolute_error')\n",
    "    cv_scores.append({'cv_score': -np.mean(cv_score), 'n_est': n_est, 'lr': lr})\n",
    "    print(cv_scores[-1])\n",
    "    \n",
    "min(cv_scores, key=lambda x: x['cv_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "def pick_n_est_lr(n_est_range, lr_range, n):\n",
    "    n_est_set = np.random.randint(*n_est_range, n)\n",
    "    lr_set = np.random.uniform(*lr_range, n)\n",
    "    \n",
    "    cv_score = []\n",
    "    \n",
    "    cachedir = mkdtemp()\n",
    "    for n_est, lr in zip(n_est_set, lr_set):\n",
    "        xgbr = make_pipeline(SimpleImputer(), \n",
    "                             XGBRegressor(n_estimators=n_est, learning_rate=lr, random_state=seed),\n",
    "                             memory=cachedir)\n",
    "        cv_score = cross_val_score(xgbr, X, y, cv=5,\n",
    "                                   scoring='neg_mean_absolute_error')\n",
    "        cv_scores.append({'cv_score': -np.mean(cv_score), 'n_est': n_est, 'lr': lr})\n",
    "    rmtree(cachedir)\n",
    "    \n",
    "    return min(cv_scores, key=lambda x: x['cv_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = tn.pick_xgbr_hyperparams(X, y, (100, 1000), (.1, .2), 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = (make_pipeline(SimpleImputer(),\n",
    "                             XGBRegressor(n_estimators=842, learning_rate=.05377, random_state=seed))\n",
    "               .fit(X,y)\n",
    "               .predict(X_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'Id': X_eval.Id.astype(int), 'SalePrice': y_pred_test})\n",
    "out.to_csv('xgbr_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgbt = GradientBoostingRegressor(n_estimators=300, loss='lad')\n",
    "bgbt.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['LotArea', 'BedroomAbvGr', 'OverallCond', 'TotRmsAbvGrd']\n",
    "features_indices = [X_t.columns.get_loc(f) for f in important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_dependence(bgbt, X_t, [0,1], important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partial_dependence(bgbt, X_t, [2,3], important_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
